
		      +---------------------------------------+
		      |  COP6616 Parallel Computing Project 2 |
		      +---------------------------------------+
		      |	     By Joseph Allen on 3/05/20       |
		      +----------------------------------------


+----------+
| Overview |
+----------+
In this project I wrote multiple C functions of various time complexities and 
then parallelized them using OpenMP. I then evaluated the benefits of parallel
programming by comparing executions times of these functions for varying amounts
of data and varying numbers of processors.

+-------------+
| Source Code |
+-------------+
The code for this project is all included in the single file named parallel.c.
I've included a makefile for compiling this source code with the necessary 
libraries. To run this program, simply run make and then run the resulting 
executable. Note that the executable takes two command line arguments: 
	1) option: the option/function to run
		- 0 = return the first element in the array
		- 1 = linear search for a random item in the array
		- 2 = bubble sort the array
		- 3 = perform matrix multiplication 
	2) N: the size/length of the array to test on (NxN for matrices)
This program will execute the desired option and return the execution time.
The number of threads to use and whether the results of the functions should be
printed can be changed by modifying the two variables at the beginning of the
program; NUM_THREADS and printout. By default, the number of threads is 8 and 
printout is set to false.

+------------------------+
| Experiments & Analysis |
+------------------------+
To test these functions, I tested each one on various sizes of data (from 100 to
1000000) and various numbers of threads (from 1 to 64) on a 64 thread SMP 
machine called Atlas. The results of these expirements are included in 
results.ods. For each data size, I've highlighted in yellow the number of 
threads which achieved the best results for each function.

These highlighted best results show that increasing the number of threads does
increase performance, but only up to a certain point. That point is largely 
determined by the time complexity of the underlying problem. For example,
the optimal number of threads increases very slowly for the linear search
function, but increases quicker for bubble sort and the fastest for matrix
multiplication. For arrays of size 10000 and the matrix of size 100x100 (10,000
flattened), the optimal number of threads for linear search, bubble sort, and 
matrix multiplication was 1,8, and 16, respectively. 

Clealy the return first function does not follow this trend since it has
constant time complexity and always finishes instantly (depending on the 
overhead of setting up unnecessary threads).

In conclusion, these results follow the patterns discovered in Project 1 with 
Java, except there is much less noise since I am using OpenMP instead of Java
sockets. With Java sockets, there is a lot of overhead associated with setting
up connections and transmitting data to each processor. 


				   +---------+
				   | The End |
				   +---------+



